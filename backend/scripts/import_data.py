#!/usr/bin/env python3
"""
Clauson Etimoloji SÃ¶zlÃ¼ÄŸÃ¼ - Veri Ä°mport Scripti
JSON dosyalarÄ±nÄ± PostgreSQL veritabanÄ±na yÃ¼kler.
GeliÅŸmiÅŸ normalizasyon, etimoloji aÃ§Ä±lÄ±mÄ± ve arama anahtarlarÄ± iÃ§erir.
"""
import json
import os
import sys
import re
from pathlib import Path
from typing import Dict, List, Any, Tuple
import psycopg2
from psycopg2.extras import execute_batch

# VeritabanÄ± baÄŸlantÄ± bilgileri
DB_CONFIG = {
    'dbname': os.getenv('DB_NAME', 'clauson_db'),
    'user': os.getenv('DB_USER', 'postgres'),
    'password': os.getenv('DB_PASSWORD', 'postgres'),
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', '5432')
}

# Etimoloji AÃ§Ä±lÄ±mlarÄ±
ETYMOLOGY_MAP = {
    'D': 'Derived',
    'Basic': 'Basic',
    'VU': 'Verbum Unicum',
    'S': 'See',
    'F': 'Foreign Loan Word',
    'E': 'Error',
    'PU': 'Problematical/Uncertain',
    'C': 'Chinese',
    'VUD': 'Verbum Unicum, Derived',
    'DF': 'Derived, Foreign',
    'PUD': 'Problematical, Derived',
    'VUF': 'Verbum Unicum, Foreign',
    'PUF': 'Problematical, Foreign',
    'SF': 'See, Foreign',
    'DC': 'Derived, Chinese'
}

def expand_etymology(code: str) -> str:
    """Etimoloji kodunu geniÅŸletir"""
    if not code:
        return ''
    
    # Bilinen kodlar
    if code in ETYMOLOGY_MAP:
        return ETYMOLOGY_MAP[code]
    
    # ? iÅŸaretli kodlar (Ã¶rn: ?D -> Derived?)
    clean_code = code.replace('?', '')
    if clean_code in ETYMOLOGY_MAP:
        return f"{ETYMOLOGY_MAP[clean_code]}?"
        
    return code

def normalize_word(word: str) -> str:
    """
    Kelimeyi sÄ±ralama ve arama iÃ§in normalize eder.
    1. SayÄ±larÄ± ve baÅŸtaki boÅŸluklarÄ± kaldÄ±rÄ±r.
    2. Ã–zel iÅŸaretleri (:, -, *, ?) kaldÄ±rÄ±r.
    3. Fonetik karakterleri standart TÃ¼rkÃ§e harflere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼r.
    4. KÃ¼Ã§Ã¼k harfe Ã§evirir.
    """
    if not word:
        return ''

    # 1. SayÄ±larÄ± ve baÅŸtaki boÅŸluklarÄ± temizle (Ã¶rn: "1 aÄŸ" -> "aÄŸ")
    # Sadece baÅŸtaki sayÄ±larÄ± temizliyoruz
    normalized = re.sub(r'^\d+\s*', '', word)

    # 2. Ã–zel iÅŸaretleri kaldÄ±r
    remove_chars = [':', '-', '*', '?', "'", '(', ')', '[', ']', '/', ',', '.', ';']
    for char in remove_chars:
        normalized = normalized.replace(char, '')

    # 3. Fonetik karakter haritasÄ± (Standart TÃ¼rkÃ§e harfler KORUNUR)
    replacements = {
        'Ä°': 'i', 'I': 'Ä±', 
        'Ã±': 'n', 'Å‹': 'n',  # n varyantlarÄ±
        'á¸': 'd', 'á¸': 'd',  # d varyantlarÄ±
        'Ã©': 'e',            # e varyantÄ±
        'Ä': 'a', 'Ä«': 'i', 'Å«': 'u', # Uzun Ã¼nlÃ¼ler (varsa)
        ' ': '' # BoÅŸluklarÄ± kaldÄ±r (bitiÅŸik sÄ±ralama iÃ§in)
    }

    normalized = normalized.lower()
    for old, new in replacements.items():
        normalized = normalized.replace(old.lower(), new)

    return normalized

def get_search_keywords(word: str) -> List[str]:
    """
    Ã‡oklu kelime giriÅŸleri iÃ§in arama anahtarlarÄ±nÄ± oluÅŸturur.
    Ã–rn: "Ã¶tenÃ§/1 Ã¶tÃ¼nÃ§" -> ["Ã¶tenÃ§", "Ã¶tÃ¼nÃ§"] (normalize edilmiÅŸ halleriyle)
    """
    if not word:
        return []
    
    # Slash ile ayrÄ±lmÄ±ÅŸ varyantlarÄ± bÃ¶l
    parts = word.split('/')
    keywords = []
    
    for part in parts:
        norm = normalize_word(part.strip())
        if norm and norm not in keywords:
            keywords.append(norm)
            
    return keywords

def get_occurrence_number(word: str, word_count: Dict[str, int]) -> int:
    """Kelime iÃ§in occurrence number'Ä± hesapla (1 olug, 2 olug, etc.)"""
    # Kelimenin "ana" kÄ±smÄ±nÄ± al (sayÄ±sÄ±z hali)
    base_word = re.sub(r'^\d+\s*', '', word).strip()
    word_count[base_word] = word_count.get(base_word, 0) + 1
    return word_count[base_word]

def import_json_files(json_dir: Path, conn) -> Dict[str, Any]:
    """JSON dosyalarÄ±nÄ± veritabanÄ±na yÃ¼kle"""

    cursor = conn.cursor()
    word_count = {}  # Kelime tekrar sayacÄ±
    stats = {
        'total_files': 0,
        'total_entries': 0,
        'total_variants': 0,
        'errors': []
    }

    # TÃ¼m JSON dosyalarÄ±nÄ± bul
    json_files = sorted(json_dir.rglob("*.json"))
    print(f"ğŸ“ {len(json_files)} JSON dosyasÄ± bulundu\n")

    # Her dosyayÄ± iÅŸle
    for json_file in json_files:
        try:
            with open(json_file, 'r', encoding='utf-8') as f:
                data = json.load(f)

            stats['total_files'] += 1

            # Her kelime girdisini iÅŸle
            for entry in data:
                try:
                    word = entry.get('word', '').strip()
                    if not word:
                        continue

                    # Occurrence number'Ä± hesapla
                    occurrence_num = get_occurrence_number(word, word_count)
                    
                    # Normalizasyon ve Arama AnahtarlarÄ±
                    # Ã‡oklu kelime varsa (Ã¶tenÃ§/Ã¶tÃ¼nÃ§), ilki sÄ±ralama iÃ§in baz alÄ±nÄ±r
                    primary_word_part = word.split('/')[0].strip()
                    normalized_word = normalize_word(primary_word_part)
                    
                    search_keywords = get_search_keywords(word)
                    
                    # Etimoloji geniÅŸletme
                    etym_code = entry.get('etymology_type', '')
                    etym_full = expand_etymology(etym_code)

                    # Kelimeyi ekle
                    # Not: Åema deÄŸiÅŸikliÄŸi gerekecek (normalized_word, search_keywords)
                    # Bu script ÅŸimdilik mevcut ÅŸemaya uydurarak Ã§alÄ±ÅŸacak, 
                    # ancak normalized_word'Ã¼ word_normalized sÃ¼tununa yazacaÄŸÄ±z.
                    cursor.execute("""
                        INSERT INTO words (
                            word, word_normalized, search_keywords, meaning, etymology_type,
                            cross_reference, full_entry_text, occurrence_number
                        ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s)
                        RETURNING id
                    """, (
                        word,
                        normalized_word, # word_normalized sÃ¼tununu kullanÄ±yoruz
                        search_keywords, # Arama anahtarlarÄ± listesi
                        entry.get('meaning', ''),
                        etym_full, # GeniÅŸletilmiÅŸ etimoloji
                        entry.get('cross_reference', ''),
                        entry.get('full_entry_text', ''),
                        occurrence_num
                    ))

                    word_id = cursor.fetchone()[0]
                    stats['total_entries'] += 1

                    # VaryantlarÄ± ekle
                    variants = entry.get('variants', [])
                    if variants:
                        for variant in variants:
                            if variant and variant.strip():
                                cursor.execute("""
                                    INSERT INTO variants (word_id, variant, variant_normalized)
                                    VALUES (%s, %s, %s)
                                """, (
                                    word_id,
                                    variant.strip(),
                                    normalize_word(variant.strip())
                                ))
                                stats['total_variants'] += 1
                    
                    # Arama anahtarlarÄ±nÄ± varyantlar tablosuna "gizli varyant" olarak ekleyebiliriz
                    # veya ayrÄ± bir tablo gerekebilir. Åimdilik varyantlara ekleyelim ki arama Ã§alÄ±ÅŸsÄ±n.
                    # Ancak bu "search_keywords" listesindeki kelimeler zaten varyantlarda olabilir.
                    # Kontrol edip ekleyelim.
                    for keyword in search_keywords:
                        # Bu keyword zaten ana kelime veya varyant mÄ±?
                        is_existing = (keyword == normalized_word) or \
                                      any(normalize_word(v) == keyword for v in variants)
                        
                        if not is_existing:
                            # Ekstra arama anahtarÄ± olarak varyantlara ekle (gÃ¶rÃ¼nmez varyant?)
                            # Åimdilik varyant olarak eklemiyoruz, Ã§Ã¼nkÃ¼ veri kirliliÄŸi yaratabilir.
                            # KullanÄ±cÄ± "veriyi bozma" dedi.
                            # Arama mantÄ±ÄŸÄ± backend tarafÄ±nda normalized_word Ã¼zerinden LIKE ile yapÄ±lmalÄ±.
                            pass

                    # Her 100 giriÅŸte bir commit (performans iÃ§in)
                    if stats['total_entries'] % 100 == 0:
                        conn.commit()
                        print(f"âœ“ {stats['total_entries']} giriÅŸ iÅŸlendi...", end='\r')

                except Exception as e:
                    stats['errors'].append(f"Kelime hatasÄ± ({word}): {str(e)}")
                    continue

            # Dosya tamamlandÄ±
            conn.commit()

        except json.JSONDecodeError:
            stats['errors'].append(f"JSON parse hatasÄ±: {json_file.name}")
            continue
        except Exception as e:
            stats['errors'].append(f"Dosya hatasÄ± ({json_file.name}): {str(e)}")
            continue

    # Son commit
    conn.commit()
    cursor.close()

    return stats

def create_indexes(conn):
    """Ä°ndeksleri oluÅŸtur (ÅŸemada tanÄ±mlÄ±)"""
    print("\nğŸ“Š Ä°ndeksler kontrol ediliyor...")
    cursor = conn.cursor()

    # Ä°ndeks sayÄ±sÄ±nÄ± kontrol et
    cursor.execute("""
        SELECT COUNT(*) FROM pg_indexes
        WHERE tablename IN ('words', 'variants')
    """)
    index_count = cursor.fetchone()[0]
    print(f"âœ“ {index_count} indeks bulundu")

    cursor.close()

def print_statistics(conn):
    """VeritabanÄ± istatistiklerini gÃ¶ster"""
    cursor = conn.cursor()

    print("\n" + "=" * 70)
    print("ğŸ“Š VERÄ°TABANI Ä°STATÄ°STÄ°KLERÄ°")
    print("=" * 70)

    # Temel istatistikler
    # word_statistics view'Ä± varsa kullan, yoksa manuel hesapla
    try:
        cursor.execute("SELECT * FROM word_statistics")
        stats = cursor.fetchone()
        if stats:
            print(f"\nâœ“ Benzersiz kelime sayÄ±sÄ±: {stats[0]:,}")
            print(f"âœ“ Toplam giriÅŸ sayÄ±sÄ±: {stats[1]:,}")
            print(f"âœ“ Etimoloji tipi sayÄ±sÄ±: {stats[2]}")
            print(f"âœ“ Tekrarlanan kelime sayÄ±sÄ±: {stats[3]:,}")
            print(f"âœ“ Toplam varyant sayÄ±sÄ±: {stats[4]:,}")
    except:
        print("\nâš ï¸  word_statistics view'Ä± bulunamadÄ±, atlanÄ±yor.")

    # Etimoloji tipi daÄŸÄ±lÄ±mÄ±
    print(f"\nğŸ“š ETÄ°MOLOJÄ° TÄ°PÄ° DAÄILIMI (Top 10):")
    cursor.execute("""
        SELECT etymology_type, COUNT(*) as count
        FROM words
        GROUP BY etymology_type
        ORDER BY count DESC
        LIMIT 10
    """)
    for row in cursor.fetchall():
        print(f"   {row[0]:20s}: {row[1]:4d}")

    # En Ã§ok tekrarlanan kelimeler
    print(f"\nğŸ”„ EN Ã‡OK TEKRARLANAN KELÄ°MELER (Top 5):")
    cursor.execute("""
        SELECT word, MAX(occurrence_number) as max_occurrence
        FROM words
        GROUP BY word
        HAVING MAX(occurrence_number) > 1
        ORDER BY max_occurrence DESC, word
        LIMIT 5
    """)
    for row in cursor.fetchall():
        print(f"   '{row[0]}': {row[1]} kez")

    print("\n" + "=" * 70)

    cursor.close()

def main():
    """Ana fonksiyon"""
    print("=" * 70)
    print("ğŸš€ CLAUSON ETÄ°MOLOJÄ° SÃ–ZLÃœÄÃœ - VERÄ° Ä°MPORT")
    print("=" * 70)
    print()

    # JSON dizini
    default_data_dir = Path(__file__).resolve().parents[2] / "Datas"
    json_dir = Path(os.getenv("DATA_DIR", default_data_dir)).resolve()
    if not json_dir.exists():
        print(f"âŒ Hata: {json_dir} bulunamadÄ±!")
        sys.exit(1)

    # VeritabanÄ±na baÄŸlan
    try:
        print(f"ğŸ”Œ VeritabanÄ±na baÄŸlanÄ±lÄ±yor ({DB_CONFIG['host']}:{DB_CONFIG['port']})...")
        conn = psycopg2.connect(**DB_CONFIG)
        print("âœ“ BaÄŸlantÄ± baÅŸarÄ±lÄ±\n")
    except Exception as e:
        print(f"âŒ VeritabanÄ± baÄŸlantÄ± hatasÄ±: {e}")
        print("\nğŸ’¡ Ä°pucu: PostgreSQL'in Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan ve DB_CONFIG'in doÄŸru olduÄŸundan emin olun")
        sys.exit(1)

    try:
        # Mevcut verileri temizle
        print("ğŸ—‘ï¸  Mevcut veriler temizleniyor...")
        cursor = conn.cursor()
        cursor.execute("TRUNCATE words, variants RESTART IDENTITY CASCADE")
        conn.commit()
        cursor.close()
        print("âœ“ Temizlendi\n")

        # JSON dosyalarÄ±nÄ± import et
        print("ğŸ“¥ JSON dosyalarÄ± import ediliyor...")
        stats = import_json_files(json_dir, conn)

        print(f"\n\nâœ… Import tamamlandÄ±!")
        print(f"   Dosya sayÄ±sÄ±: {stats['total_files']}")
        print(f"   GiriÅŸ sayÄ±sÄ±: {stats['total_entries']:,}")
        print(f"   Varyant sayÄ±sÄ±: {stats['total_variants']:,}")

        if stats['errors']:
            print(f"\nâš ï¸  {len(stats['errors'])} hata oluÅŸtu:")
            for error in stats['errors'][:5]:  # Ä°lk 5 hatayÄ± gÃ¶ster
                print(f"   - {error}")

        # Ä°ndeksleri kontrol et
        create_indexes(conn)

        # Ä°statistikleri gÃ¶ster
        print_statistics(conn)

    except Exception as e:
        print(f"\nâŒ Beklenmeyen hata: {e}")
        conn.rollback()
        sys.exit(1)
    finally:
        conn.close()
        print("\nğŸ‘‹ BaÄŸlantÄ± kapatÄ±ldÄ±")

if __name__ == "__main__":
    main()
